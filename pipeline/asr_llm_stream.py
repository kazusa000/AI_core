# pipeline/asr_llm_stream.py
from __future__ import annotations

import threading
import queue
from typing import List

from src.recorder import Recorder, RecorderConfig
from src.asr.factory import create_asr
from src.asr.base import ASRResult

from src.llm.factory import create_llm  # ç”¨æ–°ç‰ˆ factory
from src.llm.base import (
    LLMMessage,
    MessagePart,
    CancelToken,
    CancelledError,
)

class LatestQueue:
    def __init__(self, maxsize: int = 1) -> None:
        self._q: "queue.Queue[str]" = queue.Queue(maxsize=maxsize)

    def push(self, item: str) -> None:
        while True:
            try:
                self._q.put_nowait(item)
                return
            except queue.Full:
                try:
                    self._q.get_nowait()
                except queue.Empty:
                    return

    def pop(self, timeout: float = 0.2) -> str | None:
        try:
            return self._q.get(timeout=timeout)
        except queue.Empty:
            return None


class InterruptController:
    def __init__(self) -> None:
        self._lock = threading.Lock()
        self._token: CancelToken | None = None

    def new_token(self) -> CancelToken:
        with self._lock:
            if self._token is not None:
                self._token.cancel()
            self._token = CancelToken()
            return self._token

    def cancel(self) -> None:
        with self._lock:
            if self._token is not None:
                self._token.cancel()


def main():
    # =========================
    # 1) ASR
    # =========================
    asr = create_asr("paraformer")  # "whisper" or "paraformer"

    # =========================
    # 2) LLM (Gemini)
    # =========================
    # åªä»ç¯å¢ƒå˜é‡è¯»å– keyï¼Œåˆ«å†™æ­»
    # export GEMINI_API_KEY="..."
    llm = create_llm("qwen_official")  # "gemini" or "qwen_official"

    system_prompt = "ä½ æ‰®æ¼”äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯´è¯ç¬¦åˆè§’è‰²é£æ ¼ï¼Œä¸è¦è¾“å‡º markdownï¼Œä¸è¦è¾“å‡ºå¤šä½™æ ¼å¼ã€‚"

    history: List[LLMMessage] = [
        LLMMessage(role="system", parts=[MessagePart(type="text", text=system_prompt)])
    ]
    MAX_TURNS = 10  # ä¿ç•™æœ€è¿‘ 10 è½®å¯¹è¯ï¼ˆuser+assistant ç®—ä¸€è½®ï¼‰

    # =========================
    # 3) Recorder segmenting
    # =========================
    recorder = Recorder(
        RecorderConfig(
            sample_rate=16000,
            frame_ms=20,
            enable_segmenter=True,
        )
    )

    # =========================
    # 4) å¹¶å‘ï¼šASR çº¿ç¨‹æŒç»­äº§å‡º user_text
    # =========================
    user_q = LatestQueue[str]()  # åªä¿ç•™æœ€æ–°ä¸€å¥ï¼ˆæ‰“æ–­è¯­ä¹‰æ›´è‡ªç„¶ï¼‰
    stop_event = threading.Event()

    def trim_history(hist: List[LLMMessage]) -> List[LLMMessage]:
        keep = 1 + 2 * MAX_TURNS
        if len(hist) > keep:
            return [hist[0]] + hist[-(keep - 1) :]
        return hist

    # å…±äº«çŠ¶æ€ï¼šç”¨äºæ‰“æ–­å½“å‰ LLM stream
    interrupt = InterruptController(CancelToken)

    def asr_listener_loop():
        print("ğŸ§ Always listening...ï¼ˆCtrl+C é€€å‡ºï¼‰")
        print("è¯´è¯ â†’ åœé¡¿ â†’ ASR â†’ Geminiï¼ˆæµå¼ï¼‰å›å¤ï¼›æ–°è¯´è¯ä¼šæ‰“æ–­å½“å‰ç”Ÿæˆ\n")

        while not stop_event.is_set():
            audio = recorder.listen()
            duration = len(audio) / recorder.sample_rate

            if duration < 0.25:
                continue

            res: ASRResult = asr.transcribe(audio, sample_rate=recorder.sample_rate)
            user_text = (res.text or "").strip()
            if not user_text:
                continue

            lang = res.lang or "-"
            backend = res.backend or asr.__class__.__name__
            print(f"\n[{backend}] [lang={lang}] {user_text}")

            # ä¸€æ—¦æœ‰æ–°è¾“å…¥ï¼šç«‹åˆ»æ‰“æ–­æ­£åœ¨ç”Ÿæˆçš„ LLM
            interrupt.cancel()
            user_q.push(user_text)

    def llm_loop():
        nonlocal history
        while not stop_event.is_set():
            user_text = user_q.pop(timeout=0.2)
            if user_text is None:
                continue

            # æ›´æ–° history
            history.append(LLMMessage(role="user", parts=[MessagePart(type="text", text=user_text)]))
            history = trim_history(history)

            # æ–°ä¸€è½®ç”Ÿæˆï¼šåˆ›å»ºæ–° tokenï¼ˆå¹¶ cancel æ—§ tokenï¼‰
            token = interrupt.new_token()

            # æµå¼è¾“å‡º
            print("[gemini] ", end="", flush=True)
            assistant_parts: List[str] = []

            try:
                for ch in llm.stream(history, cancel_token=token):
                    if ch.text_delta:
                        print(ch.text_delta, end="", flush=True)
                        assistant_parts.append(ch.text_delta)
                    if ch.is_final:
                        break

                assistant_text = "".join(assistant_parts).strip()
                if assistant_text:
                    print("")  # æ¢è¡Œ
                    history.append(
                        LLMMessage(role="assistant", parts=[MessagePart(type="text", text=assistant_text)])
                    )
                    history = trim_history(history)
                else:
                    print("")  # æ¢è¡Œï¼ˆç©ºè¾“å‡ºä¹Ÿç»“æŸï¼‰

            except CancelledError:
                # è¢«æ–° ASR æ‰“æ–­ï¼šè¾“å‡ºä¸€è¡Œæç¤ºï¼ˆä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä¸æ‰“å°ï¼‰
                print("\n[gemini] (interrupted)")
                # ä¸æŠŠåŠæˆªå›å¤å†™å…¥ historyï¼ˆé¿å…æ±¡æŸ“ä¸Šä¸‹æ–‡ï¼‰

            except Exception as e:
                print(f"\n[gemini] (error) {e}")

    t_asr = threading.Thread(target=asr_listener_loop, daemon=True)
    t_llm = threading.Thread(target=llm_loop, daemon=True)
    t_asr.start()
    t_llm.start()

    try:
        while True:
            t_asr.join(timeout=1.0)
            t_llm.join(timeout=1.0)
    except KeyboardInterrupt:
        stop_event.set()
        interrupt.cancel()
        print("\nğŸ‘‹ bye")


if __name__ == "__main__":
    main()
